{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project: MNIST Classification using Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [1. Dataset Preprocessing](#1-dataset-preprocessing)\n",
    "- [2. Model Creation](#2-model-creation)\n",
    "- [3. Model Training, Evaluation, and Testing](#3-model-training-evaluation-and-testing)\n",
    "- [4. Fine-Tuned Model (Hyperparameter Tuning)](#4-fine-tuned-model)\n",
    "- [5. Summary and Conclusion](#5-summary-and-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project demonstrates the development of a fully connected neural network using Keras and TensorFlow for image classification tasks. We employ the classic MNIST dataset of handwritten digits (0â€“9) to walk through an end-to-end deep learning workflow.\n",
    "\n",
    "Key steps in this project include:\n",
    "- **Dataset Preprocessing:** Loading the MNIST dataset, normalizing pixel values, and splitting the data into training, validation, and test sets.\n",
    "- **Model Creation:** Building a Multi-Layer Perceptron (MLP) with an input layer, hidden layers (with dropout for regularization), Dropout layers for preventing overfitting and an output layer using Softmax activation.\n",
    "- **Training and Evaluation:** Training the model on the training data, validating its performance during training, evaluating it on unseen test data and visualizing training history and predictions.\n",
    "- **Fine-Tuning:** Using Keras Tuner for hyperparameter optimization, exploring different architectures and parameters, comparing performance between original and fine-tuned models and analyzing improvements through various metrics and visualizations.\n",
    "\n",
    "This notebook provides a comprehensive introduction to deep learning techniques with practical insights into model design, training, and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preprocessing\n",
    "\n",
    "This section focuses on preparing the MNIST dataset for training our deep learning model.\n",
    "\n",
    "First, we will load the MNIST dataset directly from Keras, which provides it pre-split into training and testing sets. We will then further split the training set into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is a classic dataset in machine learning and computer vision. It consists of grayscale images of handwritten digits from 0 to 9.\n",
    "\n",
    "*   **Number of samples**: 60,000 training images and 10,000 test images.\n",
    "*   **Number of classes**: 10 (digits 0-9).\n",
    "*   **Input dimensions**: Each image is 28x28 pixels.\n",
    "\n",
    "Let's explore the shapes and data types of our loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes and data types of training and test datasets to verify dimensions and data format\n",
    "print(\"x_train_full shape:\", x_train_full.shape)\n",
    "print(\"y_train_full shape:\", y_train_full.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"Data type of x_train_full:\", x_train_full.dtype)\n",
    "print(\"Data type of y_train_full:\", y_train_full.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we normalize the pixel values to be in the range \\[0, 1]. Currently, pixel values are integers in the range \\[0, 255]. Normalization helps in faster convergence during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to range [0, 1]\n",
    "x_train_full = x_train_full.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "print(\"Dataset normalized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a validation set from the training set. We'll use an 80-20 split for training and validation sets respectively. Since the original dataset is already split into training and test sets, we will split the original training set into training and validation sets. We will use 10,000 samples for validation and the remaining for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set out of the training data\n",
    "num_train = int(len(x_train_full) * 0.8)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "x_train, x_valid = x_train_full[:num_train], x_train_full[num_train:]\n",
    "y_train, y_valid = y_train_full[:num_train], y_train_full[num_train:]\n",
    "\n",
    "print(\"Training, validation, and test sets created.\")\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_valid shape:\", x_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualize a few training samples and their respective labels to confirm that the dataset is loaded and processed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few samples\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Creation\n",
    "\n",
    "In this section, we will build a fully connected neural network (Multi-Layer Perceptron - MLP) using Keras for image classification.\n",
    "\n",
    "Our model architecture is as follows:\n",
    "\n",
    "1. **Input Layer**: The input images (28x28 pixels) are flattened into a 1D array of 784 elements.\n",
    "2. **Hidden Layer 1**: A dense layer with 128 neurons using the ReLU activation function along with L2 regularization (lambda=0.01) to promote better generalization.\n",
    "3. **Dropout Layer**: A dropout layer with a rate of 0.2 to help prevent overfitting by randomly dropping 20% of the neurons during training.\n",
    "4. **Hidden Layer 2**: A dense layer with 64 neurons using the ReLU activation function and L2 regularization (lambda=0.01).\n",
    "5. **Dropout Layer**: Another dropout layer with a 20% drop rate.\n",
    "6. **Output Layer**: A dense layer with 10 neurons (one per class) and a softmax activation function to output probabilities for each class.\n",
    "\n",
    "Below is the Keras code used to build this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    # Flatten the 28x28 input images into a 1D array\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    # First dense layer with 128 neurons, ReLU activation and L2 regularization\n",
    "    keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.L2(0.01)),\n",
    "    # Dropout layer to prevent overfitting by randomly dropping 20% of neurons\n",
    "    keras.layers.Dropout(0.2),\n",
    "    # Second dense layer with 64 neurons, ReLU activation and L2 regularization\n",
    "    keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.L2(0.01)),\n",
    "    # Another dropout layer with 20% drop rate\n",
    "    keras.layers.Dropout(0.2),\n",
    "    # Output layer with 10 neurons (for 10 classes) and softmax activation\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Model summary to see the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compile the model. For compilation, we need to specify:\n",
    "\n",
    "*   **Loss function**: `sparse_categorical_crossentropy` is used because we have sparse labels (integers) and multiple classes.\n",
    "*   **Optimizer**: `adam` is a popular and efficient optimizer.\n",
    "*   **Metrics**: `accuracy` to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training, Evaluation, and Testing\n",
    "\n",
    "Now, we will train the model using the training data and validate it using the validation data. We will train for 20 epochs and use a batch size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "epochs = 20      # Number of complete passes through the training dataset\n",
    "batch_size = 64  # Number of samples processed before model update\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Loss, Accuracy and Training History\n",
    "\n",
    "The visualization of training history, including loss and accuracy metrics, is crucial for understanding model performance. By plotting these metrics over epochs, we can:\n",
    "\n",
    "1. Monitor Training Progress:\n",
    "   - Track how well the model is learning\n",
    "   - Identify potential overfitting or underfitting\n",
    "   - Determine optimal number of epochs\n",
    "\n",
    "2. Key Metrics to Visualize:\n",
    "   - Training Loss\n",
    "   - Validation Loss\n",
    "   - Training Accuracy\n",
    "   - Validation Accuracy\n",
    "\n",
    "3. Interpretation:\n",
    "   - Decreasing loss indicates model improvement\n",
    "   - Diverging training/validation metrics may signal overfitting\n",
    "   - Plateauing metrics suggest learning saturation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training and validation acuuracy over epochs to analyze model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Validation and Training Accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training and validation loss curves to assess model performance and detect potential overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validtation and Training Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "After training, we evaluate the model's performance on the test set to measure how well it generalizes to unseen data. Finally, we will test the model on the test set and display predictions for a few test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the model's predictions on some test samples\n",
    "\n",
    "This code snippet demonstrates how well our model performs by:\n",
    "1. Making predictions on the first 10 test images\n",
    "2. Displaying these images in a 2x5 grid\n",
    "3. Showing both the predicted label and true label for each image\n",
    "4. Using a binary colormap to display the grayscale MNIST digits\n",
    "\n",
    "The visualization helps us quickly assess if the model's predictions match the actual digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the first 10 test samples\n",
    "predictions = model.predict(x_test[:10])\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = y_test[:10]\n",
    "\n",
    "# Display predictions vs true labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap='gray')\n",
    "    color = 'green' if predicted_labels[i] == true_labels[i] else 'red'\n",
    "    plt.xlabel(f\"Pred: {predicted_labels[i]}\\nTrue: {true_labels[i]}\", color=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuned Model\n",
    "\n",
    "In this section, we will fine-tune our model using Keras Tuner to systematically search for optimal hyperparameters. We'll define a search space for various hyperparameters and let the tuner find the best combination.\n",
    "\n",
    "We define a model builder function that allows the tuner to explore:\n",
    "\n",
    "- Architecture Parameters:\n",
    "Number of units in dense layers (32 to 512 units, step of 32)\n",
    "Dropout rates (0.1 to 0.5, step of 0.1)\n",
    "L2 regularization values (0.01, 0.1, 1.0)\n",
    "\n",
    "- Training Parameters:\n",
    "Choice of optimizers (adam, sgd, rmsprop)\n",
    "\n",
    "The tuner will:\n",
    "\n",
    "- Perform random search through the hyperparameter space\n",
    "- Execute multiple trials with different combinations\n",
    "- Evaluate performance using validation accuracy\n",
    "- Select the best performing configuration\n",
    "\n",
    "We will then:\n",
    "\n",
    "- Train the best model configuration\n",
    "- Compare its performance with the original model\n",
    "- Visualize the differences in:\n",
    "  Training and validation accuracy\n",
    "  Training and validation loss\n",
    "  Prediction accuracy on test samples\n",
    "- Create detailed comparisons of model performances\n",
    "\n",
    "This systematic approach to hyperparameter tuning should help us identify a more optimal model configuration compared to our initial implementation.\n",
    "\n",
    "This section demonstrates the importance of hyperparameter tuning in deep learning and provides practical experience with automated tuning tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter space\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(units=hp.Int('dense_units_1',min_value=32,max_value=512,step=32),activation='relu',\n",
    "                     kernel_regularizer=keras.regularizers.L2(hp.Choice('l2_value_1', values=[0.01, 0.1, 1.0]))),\n",
    "        keras.layers.Dropout(rate=hp.Float('dropout_rate_1',min_value=0.1,max_value=0.5,step=0.1)),\n",
    "        keras.layers.Dense(units=hp.Int('dense_units_2',min_value=32,max_value=512,step=32),activation='relu',\n",
    "                     kernel_regularizer=keras.regularizers.L2(hp.Choice('l2_value_2', values=[0.01, 0.1, 1.0]))),\n",
    "        keras.layers.Dropout(rate=hp.Float('dropout_rate_2',min_value=0.1,max_value=0.5,step=0.1)),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning\n",
    "tuner = kt.RandomSearch(model_builder,\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=10,\n",
    "                        executions_per_trial=2,\n",
    "                        directory='hyperparameter_tuning',\n",
    "                        project_name='mnist_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of the search space \n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best hyperparameters using training and validation data\n",
    "tuner.search(x_train, y_train,\n",
    "                   epochs=10,\n",
    "                   validation_data=(x_valid, y_valid),\n",
    "                   verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a summary of the results \n",
    "tuner.results_summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters and model\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in best_hps.values.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuned Model\n",
    "fine_tuned_model=tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train fine-tuned model\n",
    "fine_tuned_history = fine_tuned_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(x_valid, y_valid),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model on the test set\n",
    "fine_tuned_test_loss, fine_tuned_test_accuracy = fine_tuned_model.evaluate(x_test, y_test)\n",
    "print(f\"Fine-tuned Test Loss: {fine_tuned_test_loss:.4f}\")\n",
    "print(f\"Fine-tuned Test Accuracy: {fine_tuned_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the fine-tuned model and compare its performance with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history of fine-tuned model\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "pd.DataFrame(fine_tuned_history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # Set the vertical range to [0-1]\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title('Fine-tuned Model Training and Validation Accuracy and Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the fine-tuned model for the first 10 test samples\n",
    "fine_tuned_predictions = fine_tuned_model.predict(x_test[:10])\n",
    "fine_tuned_predicted_labels = np.argmax(fine_tuned_predictions, axis=1)\n",
    "\n",
    "# Display predictions vs true labels for fine-tuned model\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(f\"Predicted: {fine_tuned_predicted_labels[i]}\\nTrue: {true_labels[i]}\")\n",
    "plt.title('Fine-tuned Model Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparison_data = {\n",
    "    \"Model\": [\"Original Model\", \"Fine-Tuned Model\"],\n",
    "    \"Test Accuracy\": [test_accuracy, fine_tuned_test_accuracy],\n",
    "    \"Test Loss\": [test_loss, fine_tuned_test_loss]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data,)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and compare validation loss between original and fine-tuned models\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='Original Model Val Loss')\n",
    "plt.plot(fine_tuned_history.history['val_loss'], label='Tuned Model Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Validation Loss Comparison')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and compare validation accuracy between original and fine-tuned models\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='Original Model Val Accuracy')\n",
    "plt.plot(fine_tuned_history.history['val_accuracy'], label='Tuned Model Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and compare training loss between original and fine-tuned models\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.plot(history.history['loss'], label='Original Model training loss')\n",
    "plt.plot(fine_tuned_history.history['loss'], label='Tuned Model training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss Comparison')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and compare training accuracy between original and fine-tuned models\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Original Model training accuracy')\n",
    "plt.plot(fine_tuned_history.history['accuracy'], label='Tuned Model training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title('Training Accuracy Comparison')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions comparison\n",
    "fine_tuned_predictions = fine_tuned_model.predict(x_test)\n",
    "\n",
    "print(\"\\nPredictions Comparison (First 10 Test Samples):\")\n",
    "for i in range(10):\n",
    "    original_predicted = np.argmax(predictions[i])\n",
    "    fine_tuned_predicted = np.argmax(fine_tuned_predictions[i])\n",
    "    true_label = y_test[i]\n",
    "    print(f\"Sample {i+1}: Original Predicted: {original_predicted}, Tuned Predicted: {fine_tuned_predicted}, True: {true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "To further analyze model performance, we present a confusion matrix for the test set. This helps identify which digits are most frequently misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the entire test set\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Original Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fine-tuned model, repeat with fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = np.argmax(fine_tuned_model.predict(x_test), axis=1)\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Fine-Tuned Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassified Examples\n",
    "\n",
    "Below, we display a few test images where the model's prediction did not match the true label. This can provide insights into common sources of error and potential areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified indices for the original model\n",
    "misclassified_idx = np.where(y_pred != y_test)[0]\n",
    "\n",
    "# Display a few misclassified examples\n",
    "num_to_show = 10\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i, idx in enumerate(misclassified_idx[:num_to_show]):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_test[idx], cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}, Pred: {y_pred[idx]}\", color='red')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Misclassified Examples - Original Model')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fine-tuned model, use y_pred_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_idx_tuned = np.where(y_pred_tuned != y_test)[0]\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i, idx in enumerate(misclassified_idx_tuned[:num_to_show]):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(x_test[idx], cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}, Pred: {y_pred_tuned[idx]}\", color='red')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Misclassified Examples - Fine-Tuned Model')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Conclusion\n",
    "\n",
    "In this project, we developed and fine-tuned a fully connected neural network for classifying handwritten digits from the MNIST dataset using Keras and TensorFlow. We implemented a systematic approach to model development, training, and optimization.\n",
    "\n",
    "**Key Accomplishments:**\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "    - Successfully preprocessed the MNIST dataset\n",
    "    - Implemented proper data normalization\n",
    "    - Created appropriate training, validation, and test splits\n",
    "    - Visualized sample data for better understanding\n",
    "\n",
    "2. Initial Model Development\n",
    "\n",
    "    - Implemented a multi-layer perceptron architecture\n",
    "    - Incorporated regularization techniques (L2 and dropout)\n",
    "    - Achieved baseline performance for comparison\n",
    "\n",
    "3. Hyperparameter Optimization\n",
    "\n",
    "    - Implemented systematic hyperparameter tuning using Keras Tuner\n",
    "    - Explored various model configurations including:\n",
    "    - Different network architectures (32-512 units per layer)\n",
    "    - Various dropout rates (0.1-0.5)\n",
    "    - Different optimizers (adam, sgd, rmsprop)\n",
    "    - Different L2 regularization values\n",
    "\n",
    "4. Performance Analysis\n",
    "\n",
    "    - Conducted thorough comparison between original and tuned models\n",
    "    - Visualized training and validation metrics\n",
    "    - Analyzed prediction accuracy on test samples\n",
    "\n",
    "**Potential Areas for Improvement:**\n",
    "\n",
    "1. Model Architecture\n",
    "\n",
    "    - Experiment with different network architectures\n",
    "    - Consider implementing Convolutional Neural Networks (CNNs)\n",
    "    - Try different activation functions\n",
    "\n",
    "2. Training Strategy\n",
    "\n",
    "    - Implement learning rate scheduling\n",
    "    - Explore different optimization algorithms\n",
    "    - Try different batch sizes and training durations\n",
    "\n",
    "3. Data Enhancement\n",
    "\n",
    "    - Implement data augmentation techniques\n",
    "    - Explore different preprocessing methods\n",
    "    - Consider using additional datasets\n",
    "\n",
    "4. Regularization\n",
    "\n",
    "    - Test different dropout patterns\n",
    "    - Experiment with other regularization techniques\n",
    "    - Implement early stopping strategies\n",
    "\n",
    "This project provides a solid foundation for understanding and implementing deep learning workflows for image classification. By systematically exploring the areas for improvement, you can further enhance the model's performance and gain deeper insights into deep learning practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
